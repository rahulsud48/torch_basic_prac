{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache using fc-list. This may take a moment.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LeNet Architecture with BatchNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Display the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNetBN(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet_model = LeNet()\n",
    "print(lenet_model)\n",
    "lenetBN_model = LeNetBN()\n",
    "print(lenetBN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# 3. Get MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root='data', num_workers=1):\n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # Resize to 32X32\n",
    "        transforms.Resize((32, 32)),\n",
    "        # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
    "        # This mean and variance is calculated on training data (verify yourself)\n",
    "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
    "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "    data_root: str = \"data\"  # folder to save MNIST data (default: data)\n",
    "    num_workers: int = 10  # number of concurrent processes using to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. System Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mood\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gardients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        output = model(data)\n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lowers batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 10\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        epochs_count=epoch_num_to_set,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_configuration.learning_rate\n",
    "    )\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:04, 6483376.38it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 60914.56it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423680it [00:01, 2745809.12it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 24542.90it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 0 [1600/60000] Loss: 2.313408 Acc: 0.1250\n",
      "Train Epoch: 0 [3200/60000] Loss: 2.289712 Acc: 0.1875\n",
      "Train Epoch: 0 [4800/60000] Loss: 2.225257 Acc: 0.4375\n",
      "Train Epoch: 0 [6400/60000] Loss: 2.174840 Acc: 0.1875\n",
      "Train Epoch: 0 [8000/60000] Loss: 1.579979 Acc: 0.3750\n",
      "Train Epoch: 0 [9600/60000] Loss: 1.249817 Acc: 0.5000\n",
      "Train Epoch: 0 [11200/60000] Loss: 1.201265 Acc: 0.5625\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.863446 Acc: 0.6875\n",
      "Train Epoch: 0 [14400/60000] Loss: 0.891113 Acc: 0.6875\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.521919 Acc: 0.8125\n",
      "Train Epoch: 0 [17600/60000] Loss: 0.880572 Acc: 0.5000\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.644989 Acc: 0.7500\n",
      "Train Epoch: 0 [20800/60000] Loss: 0.861194 Acc: 0.6250\n",
      "Train Epoch: 0 [22400/60000] Loss: 1.489848 Acc: 0.5625\n",
      "Train Epoch: 0 [24000/60000] Loss: 1.379102 Acc: 0.4375\n",
      "Train Epoch: 0 [25600/60000] Loss: 1.191278 Acc: 0.6250\n",
      "Train Epoch: 0 [27200/60000] Loss: 0.880160 Acc: 0.6875\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.609085 Acc: 0.6875\n",
      "Train Epoch: 0 [30400/60000] Loss: 1.066905 Acc: 0.5000\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.977467 Acc: 0.5000\n",
      "Train Epoch: 0 [33600/60000] Loss: 0.571323 Acc: 0.6875\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.551186 Acc: 0.8125\n",
      "Train Epoch: 0 [36800/60000] Loss: 0.687804 Acc: 0.6875\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.755964 Acc: 0.7500\n",
      "Train Epoch: 0 [40000/60000] Loss: 0.421006 Acc: 0.8750\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.696274 Acc: 0.6875\n",
      "Train Epoch: 0 [43200/60000] Loss: 0.512269 Acc: 0.7500\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.889117 Acc: 0.8125\n",
      "Train Epoch: 0 [46400/60000] Loss: 0.622576 Acc: 0.6875\n",
      "Train Epoch: 0 [48000/60000] Loss: 1.198385 Acc: 0.6250\n",
      "Train Epoch: 0 [49600/60000] Loss: 0.544023 Acc: 0.8125\n",
      "Train Epoch: 0 [51200/60000] Loss: 1.121032 Acc: 0.5000\n",
      "Train Epoch: 0 [52800/60000] Loss: 0.316920 Acc: 0.8750\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.683496 Acc: 0.8750\n",
      "Train Epoch: 0 [56000/60000] Loss: 0.328734 Acc: 0.8125\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.577557 Acc: 0.7500\n",
      "Train Epoch: 0 [59200/60000] Loss: 0.316689 Acc: 0.9375\n",
      "Elapsed 23.60s, 23.60 s/epoch, 0.01 s/batch, ets 212.43s\n",
      "\n",
      "Test set: Average loss: 0.6023, Accuracy: 7663/10000 (77%)\n",
      "\n",
      "Train Epoch: 1 [1600/60000] Loss: 0.836053 Acc: 0.6875\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.602949 Acc: 0.6875\n",
      "Train Epoch: 1 [4800/60000] Loss: 0.398920 Acc: 0.8125\n",
      "Train Epoch: 1 [6400/60000] Loss: 1.073461 Acc: 0.8125\n",
      "Train Epoch: 1 [8000/60000] Loss: 0.562044 Acc: 0.8125\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.641519 Acc: 0.6250\n",
      "Train Epoch: 1 [11200/60000] Loss: 0.214561 Acc: 0.9375\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.464644 Acc: 0.8125\n",
      "Train Epoch: 1 [14400/60000] Loss: 0.188590 Acc: 0.9375\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.407554 Acc: 0.7500\n",
      "Train Epoch: 1 [17600/60000] Loss: 0.380315 Acc: 0.7500\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.254387 Acc: 0.9375\n",
      "Train Epoch: 1 [20800/60000] Loss: 0.499763 Acc: 0.9375\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.456379 Acc: 0.8125\n",
      "Train Epoch: 1 [24000/60000] Loss: 0.764100 Acc: 0.8750\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.627488 Acc: 0.8125\n",
      "Train Epoch: 1 [27200/60000] Loss: 0.532921 Acc: 0.8125\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.531716 Acc: 0.8750\n",
      "Train Epoch: 1 [30400/60000] Loss: 0.911307 Acc: 0.6250\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.938632 Acc: 0.6875\n",
      "Train Epoch: 1 [33600/60000] Loss: 0.218609 Acc: 0.9375\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.680710 Acc: 0.6875\n",
      "Train Epoch: 1 [36800/60000] Loss: 0.402312 Acc: 0.7500\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.483512 Acc: 0.7500\n",
      "Train Epoch: 1 [40000/60000] Loss: 0.610828 Acc: 0.7500\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.412790 Acc: 0.8125\n",
      "Train Epoch: 1 [43200/60000] Loss: 0.246664 Acc: 0.9375\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.916262 Acc: 0.6250\n",
      "Train Epoch: 1 [46400/60000] Loss: 0.498560 Acc: 0.8125\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.248756 Acc: 0.8750\n",
      "Train Epoch: 1 [49600/60000] Loss: 0.469103 Acc: 0.8750\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.326825 Acc: 0.8750\n",
      "Train Epoch: 1 [52800/60000] Loss: 0.159402 Acc: 1.0000\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.244373 Acc: 1.0000\n",
      "Train Epoch: 1 [56000/60000] Loss: 0.441914 Acc: 0.7500\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.429978 Acc: 0.8750\n",
      "Train Epoch: 1 [59200/60000] Loss: 0.642205 Acc: 0.8750\n",
      "Elapsed 47.25s, 23.62 s/epoch, 0.01 s/batch, ets 189.00s\n",
      "\n",
      "Test set: Average loss: 0.4789, Accuracy: 8176/10000 (82%)\n",
      "\n",
      "Train Epoch: 2 [1600/60000] Loss: 0.390647 Acc: 0.8125\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.397838 Acc: 0.8125\n",
      "Train Epoch: 2 [4800/60000] Loss: 0.753785 Acc: 0.6250\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.155591 Acc: 0.9375\n",
      "Train Epoch: 2 [8000/60000] Loss: 0.341786 Acc: 0.8125\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.454536 Acc: 0.8125\n",
      "Train Epoch: 2 [11200/60000] Loss: 0.356845 Acc: 0.8750\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.694827 Acc: 0.6875\n",
      "Train Epoch: 2 [14400/60000] Loss: 0.734560 Acc: 0.6875\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.801229 Acc: 0.7500\n",
      "Train Epoch: 2 [17600/60000] Loss: 0.550564 Acc: 0.9375\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.281687 Acc: 0.8750\n",
      "Train Epoch: 2 [20800/60000] Loss: 0.446995 Acc: 0.8125\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.386680 Acc: 0.7500\n",
      "Train Epoch: 2 [24000/60000] Loss: 0.654942 Acc: 0.7500\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.436832 Acc: 0.8125\n",
      "Train Epoch: 2 [27200/60000] Loss: 0.403644 Acc: 0.8750\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.803248 Acc: 0.7500\n",
      "Train Epoch: 2 [30400/60000] Loss: 0.398228 Acc: 0.7500\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.355332 Acc: 0.8750\n",
      "Train Epoch: 2 [33600/60000] Loss: 0.614148 Acc: 0.7500\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.345375 Acc: 0.8125\n",
      "Train Epoch: 2 [36800/60000] Loss: 0.093778 Acc: 0.9375\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.473063 Acc: 0.8125\n",
      "Train Epoch: 2 [40000/60000] Loss: 0.672729 Acc: 0.6875\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.404552 Acc: 0.8750\n",
      "Train Epoch: 2 [43200/60000] Loss: 0.168883 Acc: 0.9375\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.240011 Acc: 0.8125\n",
      "Train Epoch: 2 [46400/60000] Loss: 0.455398 Acc: 0.7500\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.500957 Acc: 0.7500\n",
      "Train Epoch: 2 [49600/60000] Loss: 0.312136 Acc: 0.8750\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.366696 Acc: 0.8125\n",
      "Train Epoch: 2 [52800/60000] Loss: 0.697559 Acc: 0.8125\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.244392 Acc: 0.9375\n",
      "Train Epoch: 2 [56000/60000] Loss: 0.298348 Acc: 0.8750\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.272691 Acc: 0.9375\n",
      "Train Epoch: 2 [59200/60000] Loss: 0.346933 Acc: 0.8750\n",
      "Elapsed 80.30s, 26.77 s/epoch, 0.01 s/batch, ets 187.36s\n",
      "\n",
      "Test set: Average loss: 0.4133, Accuracy: 8484/10000 (85%)\n",
      "\n",
      "Train Epoch: 3 [1600/60000] Loss: 0.447785 Acc: 0.7500\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.367991 Acc: 0.9375\n",
      "Train Epoch: 3 [4800/60000] Loss: 0.387701 Acc: 0.8125\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.439975 Acc: 0.8125\n",
      "Train Epoch: 3 [8000/60000] Loss: 0.373372 Acc: 0.9375\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.268254 Acc: 0.8750\n",
      "Train Epoch: 3 [11200/60000] Loss: 0.245887 Acc: 0.8750\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.258681 Acc: 0.9375\n",
      "Train Epoch: 3 [14400/60000] Loss: 0.436189 Acc: 0.8125\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.354869 Acc: 0.8750\n",
      "Train Epoch: 3 [17600/60000] Loss: 0.305261 Acc: 0.8750\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.238569 Acc: 0.8125\n",
      "Train Epoch: 3 [20800/60000] Loss: 0.453183 Acc: 0.8125\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.478777 Acc: 0.7500\n",
      "Train Epoch: 3 [24000/60000] Loss: 0.356394 Acc: 0.8750\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.287960 Acc: 0.9375\n",
      "Train Epoch: 3 [27200/60000] Loss: 0.345678 Acc: 0.8750\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.625153 Acc: 0.7500\n",
      "Train Epoch: 3 [30400/60000] Loss: 0.604066 Acc: 0.8125\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.178136 Acc: 0.9375\n",
      "Train Epoch: 3 [33600/60000] Loss: 0.516539 Acc: 0.6875\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.238306 Acc: 0.8750\n",
      "Train Epoch: 3 [36800/60000] Loss: 0.732335 Acc: 0.8125\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.675648 Acc: 0.6875\n",
      "Train Epoch: 3 [40000/60000] Loss: 0.379040 Acc: 0.8125\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.172322 Acc: 1.0000\n",
      "Train Epoch: 3 [43200/60000] Loss: 0.279592 Acc: 0.8125\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.366686 Acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [46400/60000] Loss: 0.255852 Acc: 0.9375\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.373925 Acc: 0.8750\n",
      "Train Epoch: 3 [49600/60000] Loss: 0.275029 Acc: 0.9375\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.211030 Acc: 0.9375\n",
      "Train Epoch: 3 [52800/60000] Loss: 0.134220 Acc: 1.0000\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.531517 Acc: 0.8125\n",
      "Train Epoch: 3 [56000/60000] Loss: 0.268649 Acc: 0.9375\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.375440 Acc: 0.8125\n",
      "Train Epoch: 3 [59200/60000] Loss: 0.856034 Acc: 0.8125\n",
      "Elapsed 112.55s, 28.14 s/epoch, 0.01 s/batch, ets 168.82s\n",
      "\n",
      "Test set: Average loss: 0.3755, Accuracy: 8652/10000 (87%)\n",
      "\n",
      "Train Epoch: 4 [1600/60000] Loss: 0.077014 Acc: 1.0000\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.150133 Acc: 0.9375\n",
      "Train Epoch: 4 [4800/60000] Loss: 0.434958 Acc: 0.8125\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.169457 Acc: 0.9375\n",
      "Train Epoch: 4 [8000/60000] Loss: 0.275621 Acc: 0.9375\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.501839 Acc: 0.8125\n",
      "Train Epoch: 4 [11200/60000] Loss: 0.616331 Acc: 0.8750\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.253575 Acc: 0.8750\n",
      "Train Epoch: 4 [14400/60000] Loss: 0.223115 Acc: 0.9375\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.107125 Acc: 0.9375\n",
      "Train Epoch: 4 [17600/60000] Loss: 0.337316 Acc: 0.8750\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.256293 Acc: 0.9375\n",
      "Train Epoch: 4 [20800/60000] Loss: 0.312969 Acc: 0.9375\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.611973 Acc: 0.8125\n",
      "Train Epoch: 4 [24000/60000] Loss: 0.401587 Acc: 0.7500\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.381207 Acc: 0.8125\n",
      "Train Epoch: 4 [27200/60000] Loss: 0.127098 Acc: 1.0000\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.259740 Acc: 0.8750\n",
      "Train Epoch: 4 [30400/60000] Loss: 0.461249 Acc: 0.8125\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.403731 Acc: 0.8750\n",
      "Train Epoch: 4 [33600/60000] Loss: 0.352787 Acc: 0.8750\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.092762 Acc: 1.0000\n",
      "Train Epoch: 4 [36800/60000] Loss: 0.250860 Acc: 0.9375\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.586190 Acc: 0.7500\n",
      "Train Epoch: 4 [40000/60000] Loss: 0.183017 Acc: 1.0000\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.244995 Acc: 0.9375\n",
      "Train Epoch: 4 [43200/60000] Loss: 0.226778 Acc: 0.8125\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.405064 Acc: 0.8125\n",
      "Train Epoch: 4 [46400/60000] Loss: 0.332236 Acc: 0.8125\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.489303 Acc: 0.8750\n",
      "Train Epoch: 4 [49600/60000] Loss: 0.531485 Acc: 0.8125\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.604363 Acc: 0.8750\n",
      "Train Epoch: 4 [52800/60000] Loss: 0.217899 Acc: 1.0000\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.251644 Acc: 0.8750\n",
      "Train Epoch: 4 [56000/60000] Loss: 0.495664 Acc: 0.8125\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.127286 Acc: 1.0000\n",
      "Train Epoch: 4 [59200/60000] Loss: 0.118730 Acc: 0.9375\n",
      "Elapsed 136.93s, 27.39 s/epoch, 0.01 s/batch, ets 136.93s\n",
      "\n",
      "Test set: Average loss: 0.3540, Accuracy: 8671/10000 (87%)\n",
      "\n",
      "Train Epoch: 5 [1600/60000] Loss: 0.397281 Acc: 0.7500\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.246476 Acc: 0.9375\n",
      "Train Epoch: 5 [4800/60000] Loss: 0.712061 Acc: 0.7500\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.589172 Acc: 0.6875\n",
      "Train Epoch: 5 [8000/60000] Loss: 0.280207 Acc: 0.9375\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.248518 Acc: 0.9375\n",
      "Train Epoch: 5 [11200/60000] Loss: 0.154177 Acc: 0.9375\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.268671 Acc: 0.9375\n",
      "Train Epoch: 5 [14400/60000] Loss: 0.227208 Acc: 0.8750\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.119590 Acc: 1.0000\n",
      "Train Epoch: 5 [17600/60000] Loss: 0.638117 Acc: 0.8125\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.266688 Acc: 0.8750\n",
      "Train Epoch: 5 [20800/60000] Loss: 0.679543 Acc: 0.8125\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.384118 Acc: 0.8750\n",
      "Train Epoch: 5 [24000/60000] Loss: 0.191178 Acc: 0.9375\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.498626 Acc: 0.8125\n",
      "Train Epoch: 5 [27200/60000] Loss: 0.796836 Acc: 0.7500\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.237811 Acc: 0.9375\n",
      "Train Epoch: 5 [30400/60000] Loss: 0.267599 Acc: 0.8125\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.272344 Acc: 0.8750\n",
      "Train Epoch: 5 [33600/60000] Loss: 0.405211 Acc: 0.8750\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.383498 Acc: 0.9375\n",
      "Train Epoch: 5 [36800/60000] Loss: 0.589459 Acc: 0.8750\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.375095 Acc: 0.8750\n",
      "Train Epoch: 5 [40000/60000] Loss: 0.366041 Acc: 0.8750\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.131193 Acc: 1.0000\n",
      "Train Epoch: 5 [43200/60000] Loss: 0.288453 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.278410 Acc: 0.8750\n",
      "Train Epoch: 5 [46400/60000] Loss: 0.569835 Acc: 0.6875\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.593200 Acc: 0.7500\n",
      "Train Epoch: 5 [49600/60000] Loss: 0.242263 Acc: 0.8750\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.147421 Acc: 0.9375\n",
      "Train Epoch: 5 [52800/60000] Loss: 0.198188 Acc: 1.0000\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.327947 Acc: 0.8750\n",
      "Train Epoch: 5 [56000/60000] Loss: 0.415069 Acc: 0.9375\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.394789 Acc: 0.8125\n",
      "Train Epoch: 5 [59200/60000] Loss: 0.261718 Acc: 0.8750\n",
      "Elapsed 162.15s, 27.02 s/epoch, 0.01 s/batch, ets 108.10s\n",
      "\n",
      "Test set: Average loss: 0.3532, Accuracy: 8710/10000 (87%)\n",
      "\n",
      "Train Epoch: 6 [1600/60000] Loss: 0.186210 Acc: 0.9375\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.063081 Acc: 1.0000\n",
      "Train Epoch: 6 [4800/60000] Loss: 0.274339 Acc: 0.8125\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.062429 Acc: 1.0000\n",
      "Train Epoch: 6 [8000/60000] Loss: 0.597097 Acc: 0.6250\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.223719 Acc: 0.9375\n",
      "Train Epoch: 6 [11200/60000] Loss: 0.277379 Acc: 0.8750\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.341398 Acc: 0.8750\n",
      "Train Epoch: 6 [14400/60000] Loss: 0.667574 Acc: 0.8125\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.167956 Acc: 0.9375\n",
      "Train Epoch: 6 [17600/60000] Loss: 0.286985 Acc: 0.8750\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.113434 Acc: 1.0000\n",
      "Train Epoch: 6 [20800/60000] Loss: 0.065395 Acc: 1.0000\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.174799 Acc: 0.9375\n",
      "Train Epoch: 6 [24000/60000] Loss: 0.273425 Acc: 0.9375\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.107445 Acc: 0.9375\n",
      "Train Epoch: 6 [27200/60000] Loss: 0.143897 Acc: 0.9375\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.266025 Acc: 0.8750\n",
      "Train Epoch: 6 [30400/60000] Loss: 0.320575 Acc: 0.7500\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.332311 Acc: 0.8750\n",
      "Train Epoch: 6 [33600/60000] Loss: 0.271328 Acc: 0.9375\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.348011 Acc: 0.8750\n",
      "Train Epoch: 6 [36800/60000] Loss: 0.128279 Acc: 1.0000\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.177238 Acc: 0.9375\n",
      "Train Epoch: 6 [40000/60000] Loss: 0.270312 Acc: 0.8750\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.221876 Acc: 0.9375\n",
      "Train Epoch: 6 [43200/60000] Loss: 0.020480 Acc: 1.0000\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.403142 Acc: 0.8750\n",
      "Train Epoch: 6 [46400/60000] Loss: 0.414543 Acc: 0.9375\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.093649 Acc: 1.0000\n",
      "Train Epoch: 6 [49600/60000] Loss: 0.182647 Acc: 0.8750\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.329208 Acc: 0.8750\n",
      "Train Epoch: 6 [52800/60000] Loss: 0.327740 Acc: 0.9375\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.070833 Acc: 1.0000\n",
      "Train Epoch: 6 [56000/60000] Loss: 0.508342 Acc: 0.8125\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.455065 Acc: 0.8750\n",
      "Train Epoch: 6 [59200/60000] Loss: 0.146473 Acc: 1.0000\n",
      "Elapsed 185.01s, 26.43 s/epoch, 0.01 s/batch, ets 79.29s\n",
      "\n",
      "Test set: Average loss: 0.3236, Accuracy: 8811/10000 (88%)\n",
      "\n",
      "Train Epoch: 7 [1600/60000] Loss: 0.143013 Acc: 0.9375\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.217884 Acc: 0.8750\n",
      "Train Epoch: 7 [4800/60000] Loss: 0.149634 Acc: 1.0000\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.436261 Acc: 0.8125\n",
      "Train Epoch: 7 [8000/60000] Loss: 0.553406 Acc: 0.7500\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.361849 Acc: 0.9375\n",
      "Train Epoch: 7 [11200/60000] Loss: 0.122609 Acc: 0.8750\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.231938 Acc: 0.9375\n",
      "Train Epoch: 7 [14400/60000] Loss: 0.147535 Acc: 0.9375\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.362665 Acc: 0.8750\n",
      "Train Epoch: 7 [17600/60000] Loss: 0.116956 Acc: 1.0000\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.280421 Acc: 0.8750\n",
      "Train Epoch: 7 [20800/60000] Loss: 0.153262 Acc: 1.0000\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.214400 Acc: 0.8750\n",
      "Train Epoch: 7 [24000/60000] Loss: 0.414955 Acc: 0.8125\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.419380 Acc: 0.8125\n",
      "Train Epoch: 7 [27200/60000] Loss: 0.338148 Acc: 0.8750\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.106591 Acc: 0.9375\n",
      "Train Epoch: 7 [30400/60000] Loss: 0.441177 Acc: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [32000/60000] Loss: 0.185227 Acc: 0.8750\n",
      "Train Epoch: 7 [33600/60000] Loss: 0.263396 Acc: 0.8750\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.059194 Acc: 1.0000\n",
      "Train Epoch: 7 [36800/60000] Loss: 0.246505 Acc: 0.8750\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.214530 Acc: 0.8750\n",
      "Train Epoch: 7 [40000/60000] Loss: 0.118772 Acc: 0.9375\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.124643 Acc: 0.9375\n",
      "Train Epoch: 7 [43200/60000] Loss: 0.051511 Acc: 1.0000\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.155944 Acc: 0.9375\n",
      "Train Epoch: 7 [46400/60000] Loss: 0.090305 Acc: 1.0000\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.265952 Acc: 0.8750\n",
      "Train Epoch: 7 [49600/60000] Loss: 0.620456 Acc: 0.7500\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.044241 Acc: 1.0000\n",
      "Train Epoch: 7 [52800/60000] Loss: 0.199793 Acc: 0.9375\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.220680 Acc: 0.9375\n",
      "Train Epoch: 7 [56000/60000] Loss: 0.257066 Acc: 0.9375\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.401547 Acc: 0.7500\n",
      "Train Epoch: 7 [59200/60000] Loss: 0.271848 Acc: 0.8750\n",
      "Elapsed 208.18s, 26.02 s/epoch, 0.01 s/batch, ets 52.05s\n",
      "\n",
      "Test set: Average loss: 0.3217, Accuracy: 8830/10000 (88%)\n",
      "\n",
      "Train Epoch: 8 [1600/60000] Loss: 0.182944 Acc: 0.9375\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.363304 Acc: 0.8750\n",
      "Train Epoch: 8 [4800/60000] Loss: 0.168242 Acc: 0.9375\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.147529 Acc: 1.0000\n",
      "Train Epoch: 8 [8000/60000] Loss: 0.297675 Acc: 0.8750\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.203923 Acc: 0.9375\n",
      "Train Epoch: 8 [11200/60000] Loss: 0.258245 Acc: 0.8750\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.172994 Acc: 0.8750\n",
      "Train Epoch: 8 [14400/60000] Loss: 0.041906 Acc: 1.0000\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.119877 Acc: 1.0000\n",
      "Train Epoch: 8 [17600/60000] Loss: 0.086490 Acc: 1.0000\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.252860 Acc: 0.8750\n",
      "Train Epoch: 8 [20800/60000] Loss: 0.302534 Acc: 0.9375\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.291350 Acc: 0.8750\n",
      "Train Epoch: 8 [24000/60000] Loss: 0.723186 Acc: 0.7500\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.759199 Acc: 0.7500\n",
      "Train Epoch: 8 [27200/60000] Loss: 0.040863 Acc: 1.0000\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.532082 Acc: 0.8125\n",
      "Train Epoch: 8 [30400/60000] Loss: 0.379345 Acc: 0.7500\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.318106 Acc: 0.9375\n",
      "Train Epoch: 8 [33600/60000] Loss: 0.568065 Acc: 0.8750\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.190925 Acc: 0.9375\n",
      "Train Epoch: 8 [36800/60000] Loss: 0.169334 Acc: 0.9375\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.208462 Acc: 0.9375\n",
      "Train Epoch: 8 [40000/60000] Loss: 0.284203 Acc: 0.9375\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.118249 Acc: 0.9375\n",
      "Train Epoch: 8 [43200/60000] Loss: 0.098860 Acc: 1.0000\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.336465 Acc: 0.8125\n",
      "Train Epoch: 8 [46400/60000] Loss: 0.421543 Acc: 0.8125\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.315657 Acc: 0.9375\n",
      "Train Epoch: 8 [49600/60000] Loss: 0.086513 Acc: 0.9375\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.210166 Acc: 0.9375\n",
      "Train Epoch: 8 [52800/60000] Loss: 0.061551 Acc: 1.0000\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.357490 Acc: 0.8125\n",
      "Train Epoch: 8 [56000/60000] Loss: 0.515834 Acc: 0.8125\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.480833 Acc: 0.8750\n",
      "Train Epoch: 8 [59200/60000] Loss: 0.158849 Acc: 1.0000\n",
      "Elapsed 231.30s, 25.70 s/epoch, 0.01 s/batch, ets 25.70s\n",
      "\n",
      "Test set: Average loss: 0.3193, Accuracy: 8814/10000 (88%)\n",
      "\n",
      "Train Epoch: 9 [1600/60000] Loss: 0.098746 Acc: 1.0000\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.457337 Acc: 0.8750\n",
      "Train Epoch: 9 [4800/60000] Loss: 0.360321 Acc: 0.8125\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.190922 Acc: 0.9375\n",
      "Train Epoch: 9 [8000/60000] Loss: 0.315977 Acc: 0.7500\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.234606 Acc: 0.8750\n",
      "Train Epoch: 9 [11200/60000] Loss: 0.364835 Acc: 0.8750\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.421080 Acc: 0.9375\n",
      "Train Epoch: 9 [14400/60000] Loss: 0.144662 Acc: 1.0000\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.113529 Acc: 0.9375\n",
      "Train Epoch: 9 [17600/60000] Loss: 0.401520 Acc: 0.9375\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.093085 Acc: 1.0000\n",
      "Train Epoch: 9 [20800/60000] Loss: 0.428680 Acc: 0.9375\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.376380 Acc: 0.8750\n",
      "Train Epoch: 9 [24000/60000] Loss: 0.189384 Acc: 0.8750\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.314943 Acc: 0.8125\n",
      "Train Epoch: 9 [27200/60000] Loss: 0.422830 Acc: 0.8125\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.519425 Acc: 0.8750\n",
      "Train Epoch: 9 [30400/60000] Loss: 0.282802 Acc: 0.8750\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.069933 Acc: 1.0000\n",
      "Train Epoch: 9 [33600/60000] Loss: 0.625123 Acc: 0.8750\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.334002 Acc: 0.7500\n",
      "Train Epoch: 9 [36800/60000] Loss: 0.307487 Acc: 0.8125\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.171161 Acc: 0.9375\n",
      "Train Epoch: 9 [40000/60000] Loss: 0.178946 Acc: 0.9375\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.088688 Acc: 1.0000\n",
      "Train Epoch: 9 [43200/60000] Loss: 0.033840 Acc: 1.0000\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.367976 Acc: 0.9375\n",
      "Train Epoch: 9 [46400/60000] Loss: 0.080534 Acc: 1.0000\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.123461 Acc: 0.9375\n",
      "Train Epoch: 9 [49600/60000] Loss: 0.315213 Acc: 0.9375\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.343994 Acc: 0.8750\n",
      "Train Epoch: 9 [52800/60000] Loss: 0.269931 Acc: 0.9375\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.370161 Acc: 0.8125\n",
      "Train Epoch: 9 [56000/60000] Loss: 0.241757 Acc: 0.8750\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.256225 Acc: 0.9375\n",
      "Train Epoch: 9 [59200/60000] Loss: 0.131653 Acc: 1.0000\n",
      "Elapsed 254.81s, 25.48 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.3088, Accuracy: 8856/10000 (89%)\n",
      "\n",
      "Total time: 256.37, Best Loss: 0.309\n",
      "Train Epoch: 0 [1600/60000] Loss: 1.921463 Acc: 0.3125\n",
      "Train Epoch: 0 [3200/60000] Loss: 1.388071 Acc: 0.5000\n",
      "Train Epoch: 0 [4800/60000] Loss: 0.788343 Acc: 0.8125\n",
      "Train Epoch: 0 [6400/60000] Loss: 1.006933 Acc: 0.8125\n",
      "Train Epoch: 0 [8000/60000] Loss: 0.952016 Acc: 0.6250\n",
      "Train Epoch: 0 [9600/60000] Loss: 0.803767 Acc: 0.6875\n",
      "Train Epoch: 0 [11200/60000] Loss: 0.923037 Acc: 0.6875\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.624062 Acc: 0.6875\n",
      "Train Epoch: 0 [14400/60000] Loss: 0.649333 Acc: 0.6250\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.464331 Acc: 0.8125\n",
      "Train Epoch: 0 [17600/60000] Loss: 0.529182 Acc: 0.8750\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.366681 Acc: 0.9375\n",
      "Train Epoch: 0 [20800/60000] Loss: 0.514722 Acc: 0.8750\n",
      "Train Epoch: 0 [22400/60000] Loss: 1.005761 Acc: 0.6250\n",
      "Train Epoch: 0 [24000/60000] Loss: 0.924990 Acc: 0.5625\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.859444 Acc: 0.5625\n",
      "Train Epoch: 0 [27200/60000] Loss: 0.463709 Acc: 0.8125\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.447170 Acc: 0.7500\n",
      "Train Epoch: 0 [30400/60000] Loss: 1.186325 Acc: 0.6250\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.900214 Acc: 0.8125\n",
      "Train Epoch: 0 [33600/60000] Loss: 0.458969 Acc: 0.8125\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.214282 Acc: 0.9375\n",
      "Train Epoch: 0 [36800/60000] Loss: 0.354168 Acc: 0.9375\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.378250 Acc: 0.8125\n",
      "Train Epoch: 0 [40000/60000] Loss: 0.344502 Acc: 0.8125\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.340182 Acc: 0.9375\n",
      "Train Epoch: 0 [43200/60000] Loss: 0.301513 Acc: 0.8125\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.543924 Acc: 0.7500\n",
      "Train Epoch: 0 [46400/60000] Loss: 0.566894 Acc: 0.7500\n",
      "Train Epoch: 0 [48000/60000] Loss: 1.052168 Acc: 0.6250\n",
      "Train Epoch: 0 [49600/60000] Loss: 0.568679 Acc: 0.8125\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.547051 Acc: 0.7500\n",
      "Train Epoch: 0 [52800/60000] Loss: 0.307265 Acc: 0.8750\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.396446 Acc: 0.8750\n",
      "Train Epoch: 0 [56000/60000] Loss: 0.171173 Acc: 0.9375\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.374743 Acc: 0.8750\n",
      "Train Epoch: 0 [59200/60000] Loss: 0.275482 Acc: 0.9375\n",
      "Elapsed 28.04s, 28.04 s/epoch, 0.01 s/batch, ets 252.33s\n",
      "\n",
      "Test set: Average loss: 0.4676, Accuracy: 8265/10000 (83%)\n",
      "\n",
      "Train Epoch: 1 [1600/60000] Loss: 0.384767 Acc: 0.8125\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.333872 Acc: 0.8750\n",
      "Train Epoch: 1 [4800/60000] Loss: 0.361499 Acc: 0.7500\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.979750 Acc: 0.8125\n",
      "Train Epoch: 1 [8000/60000] Loss: 0.387054 Acc: 0.8125\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.688136 Acc: 0.6875\n",
      "Train Epoch: 1 [11200/60000] Loss: 0.156323 Acc: 0.9375\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.386016 Acc: 0.8750\n",
      "Train Epoch: 1 [14400/60000] Loss: 0.133892 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [16000/60000] Loss: 0.114589 Acc: 1.0000\n",
      "Train Epoch: 1 [17600/60000] Loss: 0.312877 Acc: 0.9375\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.219281 Acc: 0.9375\n",
      "Train Epoch: 1 [20800/60000] Loss: 0.387334 Acc: 0.9375\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.343187 Acc: 0.9375\n",
      "Train Epoch: 1 [24000/60000] Loss: 0.405792 Acc: 0.8125\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.549934 Acc: 0.8750\n",
      "Train Epoch: 1 [27200/60000] Loss: 0.424366 Acc: 0.8125\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.554384 Acc: 0.8125\n",
      "Train Epoch: 1 [30400/60000] Loss: 0.637238 Acc: 0.7500\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.544525 Acc: 0.8125\n",
      "Train Epoch: 1 [33600/60000] Loss: 0.139446 Acc: 1.0000\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.722494 Acc: 0.6875\n",
      "Train Epoch: 1 [36800/60000] Loss: 0.262852 Acc: 0.8750\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.234409 Acc: 0.9375\n",
      "Train Epoch: 1 [40000/60000] Loss: 0.428340 Acc: 0.8125\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.211195 Acc: 0.9375\n",
      "Train Epoch: 1 [43200/60000] Loss: 0.290636 Acc: 0.9375\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.491612 Acc: 0.7500\n",
      "Train Epoch: 1 [46400/60000] Loss: 0.512267 Acc: 0.7500\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.259278 Acc: 0.8125\n",
      "Train Epoch: 1 [49600/60000] Loss: 0.267171 Acc: 0.9375\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.263768 Acc: 0.8750\n",
      "Train Epoch: 1 [52800/60000] Loss: 0.092794 Acc: 1.0000\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.196121 Acc: 0.9375\n",
      "Train Epoch: 1 [56000/60000] Loss: 0.431440 Acc: 0.8125\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.318061 Acc: 0.8750\n",
      "Train Epoch: 1 [59200/60000] Loss: 0.633318 Acc: 0.8125\n",
      "Elapsed 58.54s, 29.27 s/epoch, 0.01 s/batch, ets 234.16s\n",
      "\n",
      "Test set: Average loss: 0.3633, Accuracy: 8661/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [1600/60000] Loss: 0.343650 Acc: 0.8125\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.369678 Acc: 0.8750\n",
      "Train Epoch: 2 [4800/60000] Loss: 0.761706 Acc: 0.7500\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.097970 Acc: 1.0000\n",
      "Train Epoch: 2 [8000/60000] Loss: 0.318227 Acc: 0.8750\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.237875 Acc: 0.8750\n",
      "Train Epoch: 2 [11200/60000] Loss: 0.169997 Acc: 0.9375\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.465089 Acc: 0.8750\n",
      "Train Epoch: 2 [14400/60000] Loss: 0.832890 Acc: 0.7500\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.642881 Acc: 0.8125\n",
      "Train Epoch: 2 [17600/60000] Loss: 0.576108 Acc: 0.7500\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.299905 Acc: 0.8750\n",
      "Train Epoch: 2 [20800/60000] Loss: 0.192830 Acc: 0.8750\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.161894 Acc: 0.9375\n",
      "Train Epoch: 2 [24000/60000] Loss: 0.744979 Acc: 0.8125\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.240580 Acc: 0.8750\n",
      "Train Epoch: 2 [27200/60000] Loss: 0.144996 Acc: 0.9375\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.279847 Acc: 0.9375\n",
      "Train Epoch: 2 [30400/60000] Loss: 0.471353 Acc: 0.8750\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.374353 Acc: 0.7500\n",
      "Train Epoch: 2 [33600/60000] Loss: 0.648461 Acc: 0.7500\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.395670 Acc: 0.6875\n",
      "Train Epoch: 2 [36800/60000] Loss: 0.019761 Acc: 1.0000\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.321930 Acc: 0.8750\n",
      "Train Epoch: 2 [40000/60000] Loss: 0.386181 Acc: 0.8750\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.222483 Acc: 0.9375\n",
      "Train Epoch: 2 [43200/60000] Loss: 0.112114 Acc: 1.0000\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.250311 Acc: 0.8750\n",
      "Train Epoch: 2 [46400/60000] Loss: 0.396337 Acc: 0.8750\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.420187 Acc: 0.8750\n",
      "Train Epoch: 2 [49600/60000] Loss: 0.199645 Acc: 0.9375\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.263893 Acc: 0.9375\n",
      "Train Epoch: 2 [52800/60000] Loss: 0.346185 Acc: 0.8125\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.205456 Acc: 0.9375\n",
      "Train Epoch: 2 [56000/60000] Loss: 0.170369 Acc: 1.0000\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.247230 Acc: 0.8750\n",
      "Train Epoch: 2 [59200/60000] Loss: 0.310063 Acc: 0.8750\n",
      "Elapsed 88.00s, 29.33 s/epoch, 0.01 s/batch, ets 205.33s\n",
      "\n",
      "Test set: Average loss: 0.3308, Accuracy: 8821/10000 (88%)\n",
      "\n",
      "Train Epoch: 3 [1600/60000] Loss: 0.305113 Acc: 0.8750\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.404200 Acc: 0.8125\n",
      "Train Epoch: 3 [4800/60000] Loss: 0.339660 Acc: 0.8750\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.492472 Acc: 0.8750\n",
      "Train Epoch: 3 [8000/60000] Loss: 0.679866 Acc: 0.8125\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.105444 Acc: 0.9375\n",
      "Train Epoch: 3 [11200/60000] Loss: 0.340799 Acc: 0.8750\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.075076 Acc: 1.0000\n",
      "Train Epoch: 3 [14400/60000] Loss: 0.324088 Acc: 0.8750\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.237984 Acc: 0.8750\n",
      "Train Epoch: 3 [17600/60000] Loss: 0.251835 Acc: 0.9375\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.205800 Acc: 0.8750\n",
      "Train Epoch: 3 [20800/60000] Loss: 0.367368 Acc: 0.8125\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.383491 Acc: 0.8750\n",
      "Train Epoch: 3 [24000/60000] Loss: 0.218910 Acc: 0.8750\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.178815 Acc: 0.8750\n",
      "Train Epoch: 3 [27200/60000] Loss: 0.344824 Acc: 0.8125\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.271702 Acc: 0.9375\n",
      "Train Epoch: 3 [30400/60000] Loss: 0.548229 Acc: 0.8750\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.058248 Acc: 1.0000\n",
      "Train Epoch: 3 [33600/60000] Loss: 0.306138 Acc: 0.8125\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.191268 Acc: 0.8750\n",
      "Train Epoch: 3 [36800/60000] Loss: 0.652505 Acc: 0.8125\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.497416 Acc: 0.8125\n",
      "Train Epoch: 3 [40000/60000] Loss: 0.373397 Acc: 0.8750\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.105666 Acc: 1.0000\n",
      "Train Epoch: 3 [43200/60000] Loss: 0.205295 Acc: 0.8750\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.241004 Acc: 0.9375\n",
      "Train Epoch: 3 [46400/60000] Loss: 0.094607 Acc: 0.9375\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.147586 Acc: 0.9375\n",
      "Train Epoch: 3 [49600/60000] Loss: 0.120864 Acc: 1.0000\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.074303 Acc: 1.0000\n",
      "Train Epoch: 3 [52800/60000] Loss: 0.059369 Acc: 1.0000\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.533188 Acc: 0.8750\n",
      "Train Epoch: 3 [56000/60000] Loss: 0.215561 Acc: 0.9375\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.249311 Acc: 0.8125\n",
      "Train Epoch: 3 [59200/60000] Loss: 0.649991 Acc: 0.8750\n",
      "Elapsed 118.12s, 29.53 s/epoch, 0.01 s/batch, ets 177.17s\n",
      "\n",
      "Test set: Average loss: 0.3118, Accuracy: 8888/10000 (89%)\n",
      "\n",
      "Train Epoch: 4 [1600/60000] Loss: 0.103723 Acc: 0.9375\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.091083 Acc: 0.9375\n",
      "Train Epoch: 4 [4800/60000] Loss: 0.369300 Acc: 0.8125\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.077852 Acc: 0.9375\n",
      "Train Epoch: 4 [8000/60000] Loss: 0.220131 Acc: 0.9375\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.442298 Acc: 0.8125\n",
      "Train Epoch: 4 [11200/60000] Loss: 0.492764 Acc: 0.8750\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.149568 Acc: 1.0000\n",
      "Train Epoch: 4 [14400/60000] Loss: 0.112731 Acc: 1.0000\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.075541 Acc: 1.0000\n",
      "Train Epoch: 4 [17600/60000] Loss: 0.265265 Acc: 0.8750\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.172709 Acc: 0.9375\n",
      "Train Epoch: 4 [20800/60000] Loss: 0.218824 Acc: 0.9375\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.268884 Acc: 0.9375\n",
      "Train Epoch: 4 [24000/60000] Loss: 0.306526 Acc: 0.8125\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.337733 Acc: 0.8750\n",
      "Train Epoch: 4 [27200/60000] Loss: 0.116310 Acc: 1.0000\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.225235 Acc: 0.9375\n",
      "Train Epoch: 4 [30400/60000] Loss: 0.331117 Acc: 0.8750\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.376368 Acc: 0.8125\n",
      "Train Epoch: 4 [33600/60000] Loss: 0.261236 Acc: 0.9375\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.156566 Acc: 0.9375\n",
      "Train Epoch: 4 [36800/60000] Loss: 0.086334 Acc: 1.0000\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.537671 Acc: 0.7500\n",
      "Train Epoch: 4 [40000/60000] Loss: 0.141009 Acc: 0.9375\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.351812 Acc: 0.8750\n",
      "Train Epoch: 4 [43200/60000] Loss: 0.298079 Acc: 0.8750\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.380926 Acc: 0.8750\n",
      "Train Epoch: 4 [46400/60000] Loss: 0.461047 Acc: 0.8125\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.479737 Acc: 0.7500\n",
      "Train Epoch: 4 [49600/60000] Loss: 0.519308 Acc: 0.8125\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.322690 Acc: 0.8750\n",
      "Train Epoch: 4 [52800/60000] Loss: 0.162308 Acc: 1.0000\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.146652 Acc: 0.9375\n",
      "Train Epoch: 4 [56000/60000] Loss: 0.353769 Acc: 0.8125\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.085264 Acc: 1.0000\n",
      "Train Epoch: 4 [59200/60000] Loss: 0.082903 Acc: 1.0000\n",
      "Elapsed 148.45s, 29.69 s/epoch, 0.01 s/batch, ets 148.45s\n",
      "\n",
      "Test set: Average loss: 0.3063, Accuracy: 8879/10000 (89%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [1600/60000] Loss: 0.308825 Acc: 0.8750\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.152660 Acc: 0.9375\n",
      "Train Epoch: 5 [4800/60000] Loss: 0.537417 Acc: 0.6875\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.515989 Acc: 0.7500\n",
      "Train Epoch: 5 [8000/60000] Loss: 0.135558 Acc: 1.0000\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.317324 Acc: 0.9375\n",
      "Train Epoch: 5 [11200/60000] Loss: 0.126488 Acc: 1.0000\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.083801 Acc: 1.0000\n",
      "Train Epoch: 5 [14400/60000] Loss: 0.090845 Acc: 0.9375\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.044285 Acc: 1.0000\n",
      "Train Epoch: 5 [17600/60000] Loss: 0.560132 Acc: 0.6875\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.187455 Acc: 0.8750\n",
      "Train Epoch: 5 [20800/60000] Loss: 0.372386 Acc: 0.8125\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.252862 Acc: 0.9375\n",
      "Train Epoch: 5 [24000/60000] Loss: 0.209534 Acc: 0.9375\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.457263 Acc: 0.8750\n",
      "Train Epoch: 5 [27200/60000] Loss: 0.533612 Acc: 0.8125\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.098435 Acc: 1.0000\n",
      "Train Epoch: 5 [30400/60000] Loss: 0.256234 Acc: 0.8125\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.441049 Acc: 0.7500\n",
      "Train Epoch: 5 [33600/60000] Loss: 0.434573 Acc: 0.7500\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.256509 Acc: 0.9375\n",
      "Train Epoch: 5 [36800/60000] Loss: 0.468830 Acc: 0.8750\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.394164 Acc: 0.8125\n",
      "Train Epoch: 5 [40000/60000] Loss: 0.099500 Acc: 1.0000\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.128991 Acc: 1.0000\n",
      "Train Epoch: 5 [43200/60000] Loss: 0.194890 Acc: 0.9375\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.236170 Acc: 0.8750\n",
      "Train Epoch: 5 [46400/60000] Loss: 0.512415 Acc: 0.7500\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.319399 Acc: 0.8125\n",
      "Train Epoch: 5 [49600/60000] Loss: 0.118507 Acc: 0.9375\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.081257 Acc: 1.0000\n",
      "Train Epoch: 5 [52800/60000] Loss: 0.266738 Acc: 0.8750\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.202578 Acc: 0.9375\n",
      "Train Epoch: 5 [56000/60000] Loss: 0.360064 Acc: 0.8750\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.246977 Acc: 0.9375\n",
      "Train Epoch: 5 [59200/60000] Loss: 0.175144 Acc: 0.8750\n",
      "Elapsed 184.05s, 30.67 s/epoch, 0.01 s/batch, ets 122.70s\n",
      "\n",
      "Test set: Average loss: 0.3044, Accuracy: 8892/10000 (89%)\n",
      "\n",
      "Train Epoch: 6 [1600/60000] Loss: 0.150078 Acc: 0.9375\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.057613 Acc: 1.0000\n",
      "Train Epoch: 6 [4800/60000] Loss: 0.096435 Acc: 1.0000\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.071195 Acc: 1.0000\n",
      "Train Epoch: 6 [8000/60000] Loss: 0.310437 Acc: 0.8750\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.246165 Acc: 0.8750\n",
      "Train Epoch: 6 [11200/60000] Loss: 0.262992 Acc: 0.8750\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.197349 Acc: 0.9375\n",
      "Train Epoch: 6 [14400/60000] Loss: 0.803159 Acc: 0.6875\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.149281 Acc: 1.0000\n",
      "Train Epoch: 6 [17600/60000] Loss: 0.103642 Acc: 1.0000\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.191375 Acc: 0.8750\n",
      "Train Epoch: 6 [20800/60000] Loss: 0.112604 Acc: 0.9375\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.082673 Acc: 1.0000\n",
      "Train Epoch: 6 [24000/60000] Loss: 0.280428 Acc: 0.8750\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.093198 Acc: 0.9375\n",
      "Train Epoch: 6 [27200/60000] Loss: 0.046800 Acc: 1.0000\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.200116 Acc: 0.8750\n",
      "Train Epoch: 6 [30400/60000] Loss: 0.234307 Acc: 0.9375\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.341678 Acc: 0.9375\n",
      "Train Epoch: 6 [33600/60000] Loss: 0.164381 Acc: 0.9375\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.330309 Acc: 0.8750\n",
      "Train Epoch: 6 [36800/60000] Loss: 0.066458 Acc: 1.0000\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.174797 Acc: 0.8750\n",
      "Train Epoch: 6 [40000/60000] Loss: 0.152857 Acc: 0.9375\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.318625 Acc: 0.9375\n",
      "Train Epoch: 6 [43200/60000] Loss: 0.068417 Acc: 0.9375\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.331756 Acc: 0.8750\n",
      "Train Epoch: 6 [46400/60000] Loss: 0.141336 Acc: 0.9375\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.091486 Acc: 1.0000\n",
      "Train Epoch: 6 [49600/60000] Loss: 0.193405 Acc: 0.8750\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.245332 Acc: 0.9375\n",
      "Train Epoch: 6 [52800/60000] Loss: 0.431715 Acc: 0.8750\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.092015 Acc: 1.0000\n",
      "Train Epoch: 6 [56000/60000] Loss: 0.426813 Acc: 0.8125\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.314530 Acc: 0.8750\n",
      "Train Epoch: 6 [59200/60000] Loss: 0.123476 Acc: 1.0000\n",
      "Elapsed 214.57s, 30.65 s/epoch, 0.01 s/batch, ets 91.96s\n",
      "\n",
      "Test set: Average loss: 0.2909, Accuracy: 8944/10000 (89%)\n",
      "\n",
      "Train Epoch: 7 [1600/60000] Loss: 0.078455 Acc: 1.0000\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.196193 Acc: 0.8750\n",
      "Train Epoch: 7 [4800/60000] Loss: 0.138572 Acc: 1.0000\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.484666 Acc: 0.8125\n",
      "Train Epoch: 7 [8000/60000] Loss: 0.497545 Acc: 0.7500\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.184377 Acc: 0.9375\n",
      "Train Epoch: 7 [11200/60000] Loss: 0.127861 Acc: 0.9375\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.335684 Acc: 0.8125\n",
      "Train Epoch: 7 [14400/60000] Loss: 0.197566 Acc: 0.9375\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.249845 Acc: 0.8750\n",
      "Train Epoch: 7 [17600/60000] Loss: 0.159759 Acc: 0.9375\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.315420 Acc: 0.8750\n",
      "Train Epoch: 7 [20800/60000] Loss: 0.110880 Acc: 0.9375\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.118544 Acc: 1.0000\n",
      "Train Epoch: 7 [24000/60000] Loss: 0.482313 Acc: 0.6875\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.616193 Acc: 0.6875\n",
      "Train Epoch: 7 [27200/60000] Loss: 0.342604 Acc: 0.9375\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.106461 Acc: 0.9375\n",
      "Train Epoch: 7 [30400/60000] Loss: 0.401443 Acc: 0.8125\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.186815 Acc: 0.9375\n",
      "Train Epoch: 7 [33600/60000] Loss: 0.236699 Acc: 0.9375\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.037163 Acc: 1.0000\n",
      "Train Epoch: 7 [36800/60000] Loss: 0.172849 Acc: 0.9375\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.173747 Acc: 0.9375\n",
      "Train Epoch: 7 [40000/60000] Loss: 0.090751 Acc: 1.0000\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.075269 Acc: 1.0000\n",
      "Train Epoch: 7 [43200/60000] Loss: 0.075331 Acc: 1.0000\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.031967 Acc: 1.0000\n",
      "Train Epoch: 7 [46400/60000] Loss: 0.052554 Acc: 1.0000\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.188438 Acc: 0.9375\n",
      "Train Epoch: 7 [49600/60000] Loss: 0.303825 Acc: 0.8750\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.014216 Acc: 1.0000\n",
      "Train Epoch: 7 [52800/60000] Loss: 0.153494 Acc: 0.9375\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.231527 Acc: 0.9375\n",
      "Train Epoch: 7 [56000/60000] Loss: 0.340681 Acc: 0.8750\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.360084 Acc: 0.8750\n",
      "Train Epoch: 7 [59200/60000] Loss: 0.192236 Acc: 0.9375\n",
      "Elapsed 246.58s, 30.82 s/epoch, 0.01 s/batch, ets 61.64s\n",
      "\n",
      "Test set: Average loss: 0.2967, Accuracy: 8930/10000 (89%)\n",
      "\n",
      "Train Epoch: 8 [1600/60000] Loss: 0.179855 Acc: 0.9375\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.178396 Acc: 0.8750\n",
      "Train Epoch: 8 [4800/60000] Loss: 0.066173 Acc: 1.0000\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.152994 Acc: 0.9375\n",
      "Train Epoch: 8 [8000/60000] Loss: 0.348980 Acc: 0.8750\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.307672 Acc: 0.8750\n",
      "Train Epoch: 8 [11200/60000] Loss: 0.202976 Acc: 0.9375\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.029882 Acc: 1.0000\n",
      "Train Epoch: 8 [14400/60000] Loss: 0.164586 Acc: 0.9375\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.138311 Acc: 0.8750\n",
      "Train Epoch: 8 [17600/60000] Loss: 0.060985 Acc: 1.0000\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.161193 Acc: 0.9375\n",
      "Train Epoch: 8 [20800/60000] Loss: 0.110284 Acc: 1.0000\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.316696 Acc: 0.9375\n",
      "Train Epoch: 8 [24000/60000] Loss: 0.449946 Acc: 0.8125\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.370461 Acc: 0.8750\n",
      "Train Epoch: 8 [27200/60000] Loss: 0.029066 Acc: 1.0000\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.561943 Acc: 0.8125\n",
      "Train Epoch: 8 [30400/60000] Loss: 0.677289 Acc: 0.7500\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.185834 Acc: 0.8750\n",
      "Train Epoch: 8 [33600/60000] Loss: 0.185164 Acc: 0.9375\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.087544 Acc: 1.0000\n",
      "Train Epoch: 8 [36800/60000] Loss: 0.088807 Acc: 1.0000\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.170082 Acc: 0.9375\n",
      "Train Epoch: 8 [40000/60000] Loss: 0.154129 Acc: 1.0000\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.089567 Acc: 0.9375\n",
      "Train Epoch: 8 [43200/60000] Loss: 0.076998 Acc: 1.0000\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.266231 Acc: 0.9375\n",
      "Train Epoch: 8 [46400/60000] Loss: 0.370717 Acc: 0.8750\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.328704 Acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [49600/60000] Loss: 0.044062 Acc: 1.0000\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.171959 Acc: 0.9375\n",
      "Train Epoch: 8 [52800/60000] Loss: 0.100642 Acc: 1.0000\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.378846 Acc: 0.8750\n",
      "Train Epoch: 8 [56000/60000] Loss: 0.470369 Acc: 0.8125\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.397802 Acc: 0.8125\n",
      "Train Epoch: 8 [59200/60000] Loss: 0.163451 Acc: 0.9375\n",
      "Elapsed 277.42s, 30.82 s/epoch, 0.01 s/batch, ets 30.82s\n",
      "\n",
      "Test set: Average loss: 0.3114, Accuracy: 8893/10000 (89%)\n",
      "\n",
      "Train Epoch: 9 [1600/60000] Loss: 0.096052 Acc: 1.0000\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.205348 Acc: 0.8750\n",
      "Train Epoch: 9 [4800/60000] Loss: 0.152263 Acc: 0.9375\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.122564 Acc: 0.9375\n",
      "Train Epoch: 9 [8000/60000] Loss: 0.439723 Acc: 0.7500\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.069933 Acc: 1.0000\n",
      "Train Epoch: 9 [11200/60000] Loss: 0.245666 Acc: 0.8750\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.227590 Acc: 0.8750\n",
      "Train Epoch: 9 [14400/60000] Loss: 0.298459 Acc: 0.8750\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.107542 Acc: 0.9375\n",
      "Train Epoch: 9 [17600/60000] Loss: 0.525458 Acc: 0.8750\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.064847 Acc: 1.0000\n",
      "Train Epoch: 9 [20800/60000] Loss: 0.544379 Acc: 0.8750\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.306465 Acc: 0.8750\n",
      "Train Epoch: 9 [24000/60000] Loss: 0.114303 Acc: 1.0000\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.195657 Acc: 0.9375\n",
      "Train Epoch: 9 [27200/60000] Loss: 0.663315 Acc: 0.8125\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.510473 Acc: 0.8125\n",
      "Train Epoch: 9 [30400/60000] Loss: 0.104184 Acc: 0.9375\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.093482 Acc: 1.0000\n",
      "Train Epoch: 9 [33600/60000] Loss: 0.529806 Acc: 0.8750\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.098136 Acc: 1.0000\n",
      "Train Epoch: 9 [36800/60000] Loss: 0.226187 Acc: 0.8750\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.126564 Acc: 0.9375\n",
      "Train Epoch: 9 [40000/60000] Loss: 0.170362 Acc: 0.9375\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.082777 Acc: 1.0000\n",
      "Train Epoch: 9 [43200/60000] Loss: 0.083541 Acc: 1.0000\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.305335 Acc: 0.8750\n",
      "Train Epoch: 9 [46400/60000] Loss: 0.093563 Acc: 0.9375\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.048287 Acc: 1.0000\n",
      "Train Epoch: 9 [49600/60000] Loss: 0.022385 Acc: 1.0000\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.291440 Acc: 0.8125\n",
      "Train Epoch: 9 [52800/60000] Loss: 0.359860 Acc: 0.9375\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.308250 Acc: 0.9375\n",
      "Train Epoch: 9 [56000/60000] Loss: 0.076668 Acc: 0.9375\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.190342 Acc: 0.8750\n",
      "Train Epoch: 9 [59200/60000] Loss: 0.218715 Acc: 0.9375\n",
      "Elapsed 307.06s, 30.71 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2830, Accuracy: 8955/10000 (90%)\n",
      "\n",
      "Total time: 308.62, Best Loss: 0.283\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "modelBN = LeNetBN() \n",
    "\n",
    "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
    "\n",
    "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.Plot Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
    "\n",
    "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
    "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
    "\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above curves that when we use Batch Normalization, the training converges much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
